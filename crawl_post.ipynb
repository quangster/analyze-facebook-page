{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Page Crawling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will crawling data from Facebook Page using the library facebook-scraper\n",
    "https://github.com/kevinzg/facebook-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.facebook_scraper as fs\n",
    "import lib.utils as utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables\n",
    "First we have to define some variables that we will be using throughout the notebook\n",
    "- FANPAGE_LINK: The link to the fanpage that we want to crawl data from. This can be found by going to the fanpage and copying the link from the address bar. In this project , the link to the fanpage of [Vẽ bậy](https://www.facebook.com/vebay69) is https://www.facebook.com/vebay69. And the value of FANPAGE_LINK is \"vebay69\"\n",
    "- DATA_FOLDER_PATH: The path to the folder save data\n",
    "- COOKIE_PATH: The path to the cookie file that we will be using to authenticate with Facebook. This cookie file can be obtained by logging into Facebook and copying the cookie from the browser. For example, in Chromium, use extension [Get cookies.txt LOCALLY](https://chrome.google.com/webstore/detail/get-cookiestxt/bgaddhkoddajcdgocldbbfleckgcbcid) to get the cookie file. Then save the cookie to a file and use the path to this file as the value for COOKIE_PATH. <span style=\"color:red; font-weight:bold\">USE COOKIE FROM A FAKE ACCOUNT, OTHERWISE YOUR REAL ACCOUNT MIGHT GET BANNED.</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FANPAGE_LINK = \"vebay69\"\n",
    "DATA_FOLDER_PATH = \"data\"\n",
    "COOKIE_PATH = os.path.join('cookies', 'cookie_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set cookies\n",
    "First, set cookies to authenticate with Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.set_cookies('./cookies/cookies_1.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get profile of the FANPAGE we will crawl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use function get_page_info, timeout is the number seconds to wait before requesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_info = fs.get_page_info(\n",
    "    account=FANPAGE_LINK,\n",
    "    timeout=60,\n",
    ")\n",
    "page_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start crawling\n",
    "Firstly, we only crawl the basic information of posts like numbers of reacte, comments, sharers, text, post_id...\n",
    "After, We will crawl all the data of posts as comments_full, info of sharers, reactors in the file [Crawl comment](crawl_comment.ipynb), [Crawl sharer](crawl_sharer.ipynb), [Crawl reactor](crawl_reactor.ipynb). These notebook use post_id to extract sequence posts\n",
    "- Define number of post we want to crawl\n",
    "- Create a variable resum_post_url to store the URL of the current post for scraping data post later. This variable is saved in the file .txt with path is resume_post_url_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_POST = 10\n",
    "resume_post_url_file_path = os.path.join(DATA_FOLDER_PATH, FANPAGE_LINK, \"url\", \"resume_post_url.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a callback function to save next post url and save in a log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_pagination_url(url):\n",
    "    global resume_post_url\n",
    "    resume_post_url = url\n",
    "    print(f\"Resume url: {url}\")\n",
    "    utils.append_url_to_history(\n",
    "        url=resume_post_url,\n",
    "        file_path=os.path.join(DATA_FOLDER_PATH, FANPAGE_LINK, \"url\", \"post_url_history.txt\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read resume_post_url from path. If the file does not exist, create an empty file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_post_url = utils.read_url_file(file_path=resume_post_url_file_path)\n",
    "print(f\"Resume post url: {resume_post_url}\")\n",
    "post_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The function get_posts return an Iterator of posts, and we need to iterate through it to retrieve data for each post. \n",
    "- This function takes page_limit as a parameter, difining the number of posts to be returned. Since each request of this function fetches 10 posts, the total number of posts returned will be 10 times page_limit. \n",
    "- The options parameter defines special parameters; when specified, it will make a separate request to each individual post to retrieve specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for post in fs.get_posts(\n",
    "        account=FANPAGE_LINK,\n",
    "        page_limit=NUMBER_POST//10,\n",
    "        start_url=resume_post_url,\n",
    "        request_url_callback=handle_pagination_url,\n",
    "        options={\n",
    "            \"allow_extra_requests\": True,\n",
    "            \"reactions\": True,\n",
    "        },\n",
    "        timeout=120,\n",
    "    ):\n",
    "        print(post)\n",
    "        post_list.append(post)\n",
    "        utils.sleep(np.random.randint(5, 10))\n",
    "except fs.exceptions.TemporarilyBanned:\n",
    "    print(\"Error: Temporarily Banned\")\n",
    "\n",
    "except fs.exceptions.AccountDisabled:\n",
    "    print(\"Error: Account Disabled\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the number of crawled posts with the initially defined quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(post_list), NUMBER_POST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the resume post url and posts data with the current timestamp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.write_url_file(\n",
    "    file_path=resume_post_url_file_path,\n",
    "    url=resume_post_url,\n",
    ")\n",
    "if post_list:\n",
    "    utils.save_data(\n",
    "        data_list=post_list,\n",
    "        type=\"posts\",\n",
    "        folder_path=os.path.join(DATA_FOLDER_PATH, FANPAGE_LINK, \"raw\"),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py31082",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
